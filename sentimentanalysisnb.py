# -*- coding: utf-8 -*-
"""SentimentAnalysisNB.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MeW1f0h5_-DCYzaPKrW2kZE-Eq7eW2z7

# **Sentiment Analysis** using **Naive Bayes** algorithm

## Dataset:

The dataset that we used can be found [here](https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset/data) or you can download it from this [drive](https://drive.google.com/file/d/1PLQhHWflB3hnR1z8S8hqB9QPgHvtO10_/view?usp=drive_link). <br>
After you downloaded, upload it to this notebook.

## Code:

Function to unzip a .zip file into a folder you choose. <br>
Enter the zip file path and where you want to extract it when asked.
"""

import zipfile
import os
def unzip(pathToZip, pathToExtraction = 'Dataset'):
  os.makedirs(pathToExtraction, exist_ok=True)
  with zipfile.ZipFile(pathToZip, 'r') as zip_ref:
    zip_ref.extractall(pathToExtraction)

pathToZip = input("Path to zip: ")
pathToExtraction = input("Path to extraction: ")
unzip(pathToZip, pathToExtraction)

"""Reads a CSV file using the right encoding. <br>
 It auto-detects the file's encoding and loads it into a DataFrame.
"""

import pandas as pd
import chardet
def readCsvFile(csvFile):
  enconding = ''
  with open(f'./{pathToExtraction}/{csvFile}', 'rb') as f:
      encoding = chardet.detect(f.read(10000))['encoding']
  df = pd.read_csv(f'./{pathToExtraction}/{csvFile}', encoding=encoding)
  return df
df = readCsvFile('train.csv')
# dfBigger = readCsvFile('training.1600000.processed.noemoticon.csv')

# dfBigger.head()

"""Removes unused columns from the dataframe."""

df.drop(df.columns[[0, 4, 5, 6, 7, 8, 9]], axis=1, inplace=True)
# dfBigger.drop(dfBigger.columns[[1, 2, 3, 4]], axis=1, inplace=True)
# dfBigger.head(3)

"""Filters the tweets by sentiment (negative, neutral, or positive) and stores just the text in separate lists."""

def filterTweets(dataFrame, typeOfSentiment):
  return dataFrame[dataFrame['sentiment'].str.lower() == typeOfSentiment.lower()]['text'].tolist()

negativeTweets = filterTweets(df, 'negative')
neutralTweets = filterTweets(df, 'neutral')
positiveTweets = filterTweets(df, 'positive')

# def filterTweetsBigger(dataFrame, typeOfSentiment):
#     sentiment_col = dataFrame.columns[0]
#     text_col = dataFrame.columns[1]
#     return dataFrame[dataFrame[sentiment_col] == int(typeOfSentiment)][text_col].tolist()

# negativeTweetsBigger = filterTweetsBigger(dfBigger, '0')
# neutralTweetsBigger = filterTweetsBigger(dfBigger, '2')
# positiveTweetsBigger = filterTweetsBigger(dfBigger, '4')

# print(len(negativeTweets), len(negativeTweetsBigger))
# print(len(neutralTweets), len(neutralTweetsBigger))
# print(len(positiveTweets), len(positiveTweetsBigger))

# negativeTweets.extend(negativeTweetsBigger[:len(neutralTweets)-len(negativeTweets)])
# positiveTweets.extend(positiveTweetsBigger[:len(neutralTweets)-len(positiveTweets)])
# print(len(negativeTweets), len(positiveTweets))

"""Installs required libraries and downloads NLTK resources for text processing."""

!pip install emoji
!pip install num2words
import nltk
nltk.download('stopwords')
nltk.download('wordnet')

"""Preprocesses the tweets:
*   replaces emoji/emoticons with words;
*   cleans the tweet;
*   removes stopwords;
*   does lemmatization on the remaining words.


"""

from nltk.tokenize import TweetTokenizer
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from num2words import num2words
import re
import emoji

emoticons2 = {
     r"<3+": ":love:",
     r"[:;]\)\)+": ":laugh:",
     r"[:;][\)D]+": ":happy:",
     r"[:;][\(+\[+]": ":sad:",
     r"[:;]\*": ":kiss:",
     r"[:;]O": ":surprised:",
     r"[:;]X": ":uncomfortable:",
     r":\|": ":indifferent:",
     r":/": ":unsure:"

}

def replaceEmoticons(text):
    text =  emoji.demojize(text)
    for emoticon_regex, description in emoticons2.items():
        text = re.sub(emoticon_regex, description, text)
    return text

def cleanTweet(text):
    if isinstance(text, str) == False:
      text = ""
    text = re.sub(r"http\S+", "", text)                                     # remove URLs
    text = re.sub(r"`", "'", text)                                          # transform ` to '
    text = re.sub(r"#", "", text)                                           # remove #
    text = re.sub(r"@\w+", "", text)                                        # remove mentions
    text = replaceEmoticons(text)                                           # replace emoticons and emojis
    text = re.sub(r"\d+", lambda x: num2words(int(x.group(0))), text)       # replace numbers to numbers as words
    text = re.sub(r"[,.+!?-]", " ", text)                                   # remove punctuation: .,!?-+
    text = re.sub(r"[;:\*\)\(%&$~=]", "", text)                            # replace * after replacing emoticons (:*)
    # text = text.lower()
    return text

def preprocess(text):
  lemmatizer = WordNetLemmatizer()
  text = cleanTweet(text)
  tokens = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True).tokenize(text)
  filteredWords = [lemmatizer.lemmatize(token) for token in tokens if token not in stopwords.words("english")]
  return filteredWords

testStringPreprocessing = "i`ve :* ;* been :D ;)))))@abc sick ++++ for #yes123 the past few days??  and thus,------ my hair looks wierd. @asf:))) if ! 1124 i didn't have a ""hat"" on it would look... http://tinyurl.com/mnf4kw"

# print(preprocess(testStringPreprocessing))
print(cleanTweet(testStringPreprocessing))

# for i in range(135):
#   print(neutralTweets[i])
#   print(preprocess(neutralTweets[i]), end='\n\n')

!pip install num2words
!pip install emoji

"""Creates a dictionary for each type of

---

sentiment with every word and its frequency.
"""

def getWordsFrequency(listOfTweets):
  wordsFrequencies = {}
  for tweet in listOfTweets:
    listOfTokens = preprocess(tweet)
    for token in listOfTokens:
      if wordsFrequencies.get(token) == None:
        wordsFrequencies[str(token)] = 1
      else:
        wordsFrequencies[str(token)] += 1
  return wordsFrequencies

negativeTweetsWordsFrequency = getWordsFrequency(negativeTweets)
neutralTweetsWordsFrequency = getWordsFrequency(neutralTweets)
positiveTweetsWordsFrequency = getWordsFrequency(positiveTweets)

print("negative words freq:", len(negativeTweetsWordsFrequency))
print("neutral words freq:", len(neutralTweetsWordsFrequency))
print("positive words freq:", len(positiveTweetsWordsFrequency))

"""Creates a set of all words."""

vocab = set()
for dictionary in [negativeTweetsWordsFrequency, neutralTweetsWordsFrequency, positiveTweetsWordsFrequency]:
  vocab.update(dictionary.keys())

smoothParam = len(vocab)
print(smoothParam)

"""Calculates likelihoods and log-likelihoods for each sentiment category based on word frequencies. <br>
For calculating the likelihood probabilities it uses Laplace smoothing, so that any word encountered during testing, even if unseen in training, has a non-zero probability. <br>
In other tests, we used as smoothing parameter the number of all words used in training (see above section). <br>
In general, the smoothing parameter is a small constant, so we set it to 1 for a better accuracy.
"""

import math

def getLikelihoods(wordFrequencyDict, totalWords, smoothingParam=1):
  likelihoodProbs = {}
  for uniqueWord in vocab:
    wordCount = wordFrequencyDict.get(uniqueWord, 0)
    likelihoodProbs[uniqueWord] = (wordCount + smoothingParam) / (totalWords + smoothingParam * len(wordFrequencyDict))
  return likelihoodProbs

totalWordsNegativeTweets = sum(negativeTweetsWordsFrequency.values())
totalWordsNeutralTweets = sum(neutralTweetsWordsFrequency.values())
totalWordsPositiveTweets = sum(positiveTweetsWordsFrequency.values())

likelihoods = {
    'negative': getLikelihoods(negativeTweetsWordsFrequency, totalWordsNegativeTweets),
    'neutral':  getLikelihoods(neutralTweetsWordsFrequency, totalWordsNeutralTweets),
    'positive': getLikelihoods(positiveTweetsWordsFrequency, totalWordsPositiveTweets)
}

logLikelihoods = {}
for typeOfSentiment in ['negative', 'neutral', 'positive']:
  logLikelihoods[typeOfSentiment] = {}
  for word, prob in likelihoods[typeOfSentiment].items():
    logLikelihoods[typeOfSentiment].update({word: math.log(prob)})

"""For each type of sentiment, calculates its prior probability, then its prior log based on its probability."""

sentimentPriorProbability = {
    'negative' : len(negativeTweets)/len(df),
    'neutral' : len(neutralTweets)/len(df),
    'positive' : len(positiveTweets)/len(df)
}
sentimentPriorLog = {}
for typeOfSentiment in ['negative', 'neutral', 'positive']:
    sentimentPriorLog[typeOfSentiment] = math.log(sentimentPriorProbability[typeOfSentiment])
print(sentimentPriorProbability)
print(sentimentPriorLog)

"""Predicts the sentiment of a tweet by calculating scores for each type of sentiment based on word likelihoods and prior probabilities."""

def predictSentiments(tweet):
  tokens = preprocess(tweet)
  scores = {}
  for typeOfSentiment in ['negative', 'neutral', 'positive']:
    scores[typeOfSentiment] = sentimentPriorLog[typeOfSentiment]
    for token in tokens:
      scores[typeOfSentiment] += logLikelihoods[typeOfSentiment].get(token, 0)
  return max(scores, key=scores.get), scores

predictSentiments("http://www.dothebouncy.com/smf - some shameless plugging for the best Rangers forum on earth")

"""Testing on a dataset:"""

dfTest = readCsvFile('test.csv')
dfTest.drop(dfTest.columns[[0, 3, 4, 5, 6, 7, 8]], axis=1, inplace=True)
dfTest.head()

new_dfTest = dfTest.dropna(how='all')

correctAnswers = 0

for row in new_dfTest.itertuples():
    predictedSentiment = predictSentiments(row.text)[0]
    if row.sentiment.lower() == predictedSentiment:
        correctAnswers += 1
    else:
      print(f"Expected: {row.sentiment.lower()} | Got: {predictedSentiment} | Tweet: {row.text}")


# sizeOfMoreTests = 8000
# startIndex = 1000000
# sentimentToNum = {
#     'negative': 0,
#     'neutral' : 2 ,
#     'positive' : 4
# }
# numToSentiment = {
#     0: 'negative',
#     2: 'neutral',
#     4: 'positive'
# }

# for row in dfBigger.iloc[startIndex:(startIndex+sizeOfMoreTests)].itertuples():
#   # print(row)
#   predictedSentiment = predictSentiments(row._2)[0]
#   # print(predictedSentiment)
#   if sentimentToNum.get(predictedSentiment) != None and row._1 == sentimentToNum[predictedSentiment]:
#       correctAnswers += 1
#   else:
#       print(f"{row._1}Expected: {numToSentiment[row._1]} | Got: {predictedSentiment} | Tweet: {row._2}")

total = len(new_dfTest) # + sizeOfMoreTests
incorrectAnswers = total - correctAnswers
accuracy = (correctAnswers / total) * 100

print(f"\nCorrect: {correctAnswers}, Incorrect: {incorrectAnswers}, Accuracy: {accuracy:.2f}%")

"""# **Sentiment Analysis** using Novel approach

For this novel approach, we are using BERTweet.

We will use the same dataset as in the classic approach described above. If you haven't tested that part of the code yet, please follow the steps below:

## Preparing the Dataset:

First, download the ZIP file from [here](https://www.kaggle.com/datasets/abhi8923shriv/sentiment-analysis-dataset/data) or from this [Drive](https://drive.google.com/file/d/1PLQhHWflB3hnR1z8S8hqB9QPgHvtO10_/view?usp=drive_link) link.
After downloading it, upload the file to this notebook.

Next, we need to unzip the file. Enter the name of the .zip file, followed by the desired name for the extracted folder:
"""

import zipfile
import os
def unzip(pathToZip, pathToExtraction = 'Dataset'):
  os.makedirs(pathToExtraction, exist_ok=True)
  with zipfile.ZipFile(pathToZip, 'r') as zip_ref:
    zip_ref.extractall(pathToExtraction)

pathToZip = input("Path to zip: ")
pathToExtraction = input("Path to extraction: ")
unzip(pathToZip, pathToExtraction)

"""Preparing the function to read and modify the file (this will be use later to remove the unnecessary columns )"""

import pandas as pd
import chardet
def readCsvFile(csvFile):
  enconding = ''
  with open(f'./{pathToExtraction}/{csvFile}', 'rb') as f:
      encoding = chardet.detect(f.read(10000))['encoding']
  df = pd.read_csv(f'./{pathToExtraction}/{csvFile}', encoding=encoding)
  return df

"""## Code:

Preprocessing steps for BERTweet:

1. Clean the data: remove duplicates and links, translate emoticons and simplify the text
2. Filter the tweets to keep only those with relevant labels (positive, negative, neutral)
3. Encode the sentiment labels into numeric format (e.g., {'negative': 0, 'neutral': 1, 'positive': 2})
4. Tokenize the texts using the BERTweet tokenizer
5. Prepare a dataset compatible with PyTorch or TensorFlow for training
"""

# Imports
import torch
import torch.nn as nn
from torch.utils.data import Dataset, DataLoader
from transformers import AutoTokenizer, AutoModel #, AdamW
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
from tqdm import tqdm

import re
import emoji

# 1.Clean the data

def replace_emoticons(text):

    emoticons2 = {
     r"<3+": ":love:",
     r"[:;]\)\)+": ":laugh:",
     r"[:;][\)D]+": ":happy:",
     r"[:;][\(+\[+]": ":sad:",
     r"[:;]\*": ":kiss:",
     r"[:;]O": ":surprised:",
     r"[:;]X": ":uncomfortable:",
     r":\|": ":indifferent:",
     r":/": ":unsure:"
    }

    text =  emoji.demojize(text)
    for emoticon_regex, description in emoticons2.items():
        text = re.sub(emoticon_regex, description, text)
    return text

def minimal_clean_tweet(text):
  if isinstance(text,str):
    text = text.lower()
    text = replace_emoticons(text)
    text = re.sub(r"http\S+", "", text)  # remove URLs
    return text.strip()
  else:
    return ""

# 2. Filter the tweets

def filterTweets(dataFrame, typeOfSentiment):
  return dataFrame[dataFrame['sentiment'].str.lower() == typeOfSentiment.lower()]['text'].tolist()

negativeTweets = filterTweets(df, 'negative')
neutralTweets = filterTweets(df, 'neutral')
positiveTweets = filterTweets(df, 'positive')

!pip install emoji

from sklearn.preprocessing import LabelEncoder
from transformers import AutoTokenizer

df = readCsvFile('train.csv')
df.drop(df.columns[[0, 4, 5, 6, 7, 8, 9]], axis=1, inplace=True)

df['text'] = df['text'].apply(minimal_clean_tweet) # cleaning the dataset before use
df.drop_duplicates(subset=['text','sentiment'], inplace=True) #remove duplicates

# 3. Encode the sentiment labels into numeric format
label_encoder = LabelEncoder()
df['label'] = label_encoder.fit_transform(df['sentiment'].str.lower())

# 4.Tokenize the texts using the BERTweet tokenizer
tokenizer = AutoTokenizer.from_pretrained("vinai/bertweet-base")


train_texts,test_texts, train_labels, test_labels = train_test_split(
    df['text'].tolist(), # list with all the tweets
    df['label'].tolist(), # list with the sentiments for every tweets
    test_size = 0.2, # 20% of data will be use for test and 80% for training
    random_state = 42, # seed for a random data split
    stratify = df['label'] # make sure that the test and train data are proportional
)

# 5. Prepare a dataset compatible with PyTorch or TensorFlow for training

class TweetDataset(Dataset):
  def __init__(self,texts,labels,tokenizer,max_len=128):
    self.texts = texts
    self.labels = labels
    self.tokenizer = tokenizer
    self.max_len = max_len

  def __len__(self):
    return len(self.texts)

  def __getitem__(self,i):
    text = str(self.texts[i])
    label = self.labels[i]
    encoding = self.tokenizer(
        text,
        padding = 'max_length',
        truncation =True,
        max_length = self.max_len,
        return_tensors = 'pt'
    )
    return {
        'input_ids': encoding['input_ids'].squeeze(0),
        'attention_mask': encoding['attention_mask'].squeeze(0),
        'labels': torch.tensor(label, dtype=torch.long)
    }

# Creating DataSets
train_dataset = TweetDataset(train_texts, train_labels, tokenizer)
test_dataset = TweetDataset(test_texts, test_labels, tokenizer)

# Creating DataLoads
train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)
test_loader = DataLoader(test_dataset,batch_size=16)

print(f"Train size: {len(train_dataset)}, Test size: {len(test_dataset)}")

"""**Next Steps**
1. Load BERTweet backbone
2. Add a custom classification head
3. Training loop
4. Testing loop
"""

from torch.optim import AdamW

class TweetClassifier(nn.Module):
    def __init__(self, dropout_rate=0.3):
        super(TweetClassifier, self).__init__()
        self.backbone = AutoModel.from_pretrained("vinai/bertweet-base") # without classification head
        self.dropout = nn.Dropout(dropout_rate) # dropout layer to prevent overfitting
        self.classifier = nn.Linear(self.backbone.config.hidden_size, 3) #Linear layer to map hidden states to neutral/pos/neg
    def forward(self, input_ids, attention_mask):

        outputs = self.backbone(input_ids=input_ids, attention_mask=attention_mask)
        hidden_state = outputs.last_hidden_state
        pooled_output = hidden_state[:, 0]
        x = self.dropout(pooled_output)
        x = self.classifier(x)

        return x

"""Train & Test"""

from tqdm import tqdm  # progress bar
import torch
from sklearn.metrics import accuracy_score
from sklearn.metrics import classification_report
print(torch.cuda.is_available())  # Should print True if a GPU is available
print(torch.cuda.get_device_name(0))
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f"Using device: {device}")

# Move model to device
model = TweetClassifier()
model = model.to(device)

# train_texts_small = train_texts[:int(0.1 * len(train_texts))]
# train_labels_small = train_labels[:int(0.1 * len(train_labels))]

# test_texts_small = test_texts[:int(0.1 * len(test_texts))]
# test_labels_small = test_labels[:int(0.1 * len(test_labels))]

# # Make smaller datasets
# train_dataset_T = TweetDataset(train_texts_small, train_labels_small, tokenizer, max_len=32)
# test_dataset_T = TweetDataset(test_texts_small, test_labels_small, tokenizer, max_len=32)

# # Loaders
# train_loader_T = DataLoader(train_dataset_T, batch_size=8, shuffle=True)
# test_loader_T = DataLoader(test_dataset_T, batch_size=8)

optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)
loss_fn = nn.CrossEntropyLoss()
epochs = 3
model.train()  # set model to training mode

for epoch in range(epochs):
    total_loss = 0
    correct_predictions = 0
    # For 10% testing alternative:
    # correct_preds = 0
    # total_preds = 0

    # loop over batches
    for batch in tqdm(train_loader):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        # 1. Zero the gradients
        optimizer.zero_grad()

        # 2. Forward pass
        outputs = model(input_ids=input_ids, attention_mask=attention_mask)

        # 3. Compute loss
        loss = loss_fn(outputs, labels)
        total_loss += loss.item()

        # Current full dataset accuracy calculation
        correct_predictions += (torch.argmax(outputs, dim=1) == labels).sum().item()

        # Alternative accuracy calculation for 10% dataset reference
        # _, preds = torch.max(outputs, dim=1)
        # correct_preds += torch.sum(preds == labels).item()
        # total_preds += labels.size(0)

        # 4. Backward pass (compute gradients)
        loss.backward()

        # 5. Update weights
        optimizer.step()

    avg_loss = total_loss / len(train_loader)
    accuracy = correct_predictions / len(train_loader.dataset)  # Current calculation
    # Alternative calculation for 10% dataset
    # accuracy = correct_preds / total_preds

    print(f"Epoch {epoch+1} - Training loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}")
    print(f"Training Accuracy: {accuracy:.2f}%")


model.eval()
predictions = []
true_labels = []
correct_predictions = 0
# For 10% testing alternative:
# correct_preds = 0
# total_preds = 0

with torch.no_grad():  # disables gradient calculation
    for batch in tqdm(test_loader):
        input_ids = batch['input_ids'].to(device)
        attention_mask = batch['attention_mask'].to(device)
        labels = batch['labels'].to(device)

        outputs = model(input_ids=input_ids, attention_mask=attention_mask)

        # Get predictions
        preds = torch.argmax(outputs, dim=1)
        predictions.extend(preds.cpu().numpy())  # Add predictions
        true_labels.extend(labels.cpu().numpy())  # Add true labels

        # Current accuracy tracking
        correct_predictions += (preds == labels).sum().item()

        # Alternative accuracy tracking for 10% dataset
        # correct_preds += torch.sum(preds == labels).item()
        # total_preds += labels.size(0)
print()
# Current accuracy calculation
test_accuracy = correct_predictions / len(test_loader.dataset) * 100
print(f"Test Accuracy: {test_accuracy:.2f}%")

# Alternative accuracy calculation for 10% reference
# accuracy = correct_preds / total_preds * 100
# print(f"Test Accuracy (alternative method): {accuracy:.2f}%")

# Print classification report
print(classification_report(true_labels, predictions, target_names=label_encoder.classes_))